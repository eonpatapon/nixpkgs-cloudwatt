{ config, pkgs, lib, ... }:

with builtins;
with lib;

let
  cfg = config.infra.k8s;

  vaultRootToken = "development";
  vaultTmpfsToken = "vaulttmpfs";

  defaultVaultData = {
    "secret/kube2consul" = {
      consul_token = "dummy";
    };
  };

  defaultVaultPolicies = {
    admin = {
      "*" = {
        capabilities = [ "create" "read" "update" "delete" "list" "sudo" ];
      };
    };
    applications_token_creator = {
      "auth/token/create/applications" = {
        capabilities = [ "create" "read" "update" "delete" "list" ];
      };
      "auth/token/roles" = {
        capabilities = [ "read" "list" ];
      };
      "auth/token/roles/applications" = {
        capabilities = [ "read" ];
      };
    };
    kube2consul = {
      "secret/kube2consul" = {
        policy = "read";
      };
    };
  };

  defaultVaultRoles = {
    applications = {
      disallowed_policies = "admin";
      explicit_max_ttl = 0;
      name = "applications";
      orphan = true;
      path_suffix = "";
      period = "1h";
      renewable = true;
    };
  };

  isMaster = any (el: el == "master") cfg.roles;
  isNode = all (el: el == "node") cfg.roles;

  infraConfig = import ./config/infra_k8s.nix { inherit pkgs lib config; };
  infraLib = import ./lib/infra_k8s.nix { inherit pkgs; };

  certs = cfg.certificates;

in {

  options = {
    infra.k8s = {
      enable = mkOption {
        type = types.bool;
        default = false;
      };

      roles = mkOption {
        description = ''
          Kubernetes role that this machine should take.

          Master role will enable etcd, apiserver, scheduler and controller manager
          services. Node role will enable docker, kubelet.
        '';
        type = types.listOf (types.enum ["master" "node"]);
      };

      domain = mkOption {
        type = types.str;
        default = "dev0.loc.cloudwatt.net";
        description = ''
          Domain that will be used for the k8s cluster.
        '';
      };

      certificates = mkOption {
        type = types.attrs;
        description = ''
          A set of certificates generated by `nixos/tests/kubernetes/certs.nix`.
        '';
      };

      masterIP = mkOption {
        type = types.str;
        default = "192.168.1.1";
        description = ''
          The IP address of the machine that will run k8s api server.
        '';
      };

      seedDockerImages = mkOption {
        description = "List of docker images to preload on system";
        default = [];
        type = types.listOf types.package;
      };

      externalServices = mkOption {
        type = types.attrsOf types.attrs;
        default = {};
        description = ''
          Allow to pass a list of services definitions that will be
          integrated in the infra.

          The ip address will be added to the loopback interface.
          The service will be registered in consul.

          Example:

            {
              cassandra = {
                address = "169.254.1.100";
                port = 9160;
              };
            };

        '';
      };

      consulData = mkOption {
        type = types.attrsOf types.attrs;
        default = {};
        description = ''
          Data that will be injected in the consul server.
          Key is the path, value is an attrs that will be
          converted to JSON.
        '';
      };

      vaultData = mkOption {
        type = types.attrsOf types.attrs;
        default = {};
        description = ''
          Data that will be injected in vault at a given path.

          Example:

            {
              "secret/service" = {
                password = "foo";
              };
            };

        '';
      };

      vaultPolicies = mkOption {
        type = types.attrsOf types.attrs;
        default = {};
        description = ''
          Policies to be injected in vault.

          Example:

            service = {
              "secret/service" = {
                policy = "read";
              };
            };

        '';
      };

      vaultRoles = mkOption {
        type = types.attrsOf types.attrs;
        default = {};
        description = ''
          Roles to be injected in vault.

          Example:

            {
              my-role = {
                disallowed_policies = "admin";
                explicit_max_ttl = 0;
                name = "my-role";
                orphan = true;
                path_suffix = "";
                period = "1h";
                renewable = true;
              };
            };

        '';
      };

    };
  };

  config = mkIf cfg.enable (mkMerge [
    {
      assertions = [
        {
          assertion = all (s: hasPrefix "169.254.1" s.address) (attrValues cfg.externalServices);
          message = "externalService IP must be in the 169.254.1.x CIDR, got: ${toJSON cfg.externalServices}";
        }
      ] ++ map (o: {
        assertion = ! (isNode && (getAttr o cfg) != {});
        message = "${o} on machine '${config.networking.hostName}' of type 'node' is not supported";
      }) [ "externalServices" "consulData" "vaultData" "vaultPolicies" "vaultRoles" ];
    }

    (mkIf isMaster {

      networking = {
        interfaces.lo.ipv4.addresses = [
          { address = "169.254.1.10"; prefixLength = 32; }
          { address = "169.254.1.11"; prefixLength = 32; }
          { address = "169.254.1.12"; prefixLength = 32; }
          { address = "169.254.1.13"; prefixLength = 32; }
        ] ++ mapAttrsToList (name: { address, ... }: { inherit address; prefixLength = 32; }) cfg.externalServices;
      };

      services.etcd = {
        enable = true;
        certFile = "${certs.master}/etcd.pem";
        keyFile = "${certs.master}/etcd-key.pem";
        trustedCaFile = "${certs.master}/ca.pem";
        peerClientCertAuth = false;
        listenClientUrls = ["https://0.0.0.0:2379"];
        listenPeerUrls = ["https://0.0.0.0:2380"];
        advertiseClientUrls = ["https://etcd.${cfg.domain}:2379"];
        initialCluster = ["${config.networking.hostName}=https://etcd.${cfg.domain}:2380"];
        initialAdvertisePeerUrls = ["https://etcd.${cfg.domain}:2380"];
      };

      systemd.services.vault = {
        wantedBy = [ "multi-user.target" ];
        after = [ "network-online.target" ];
        # vault needs glibc for getent binary
        path = with pkgs; [ vault glibc waitFor ];
        script = ''
          vault server -dev -dev-listen-address=169.254.1.13:8200 -dev-root-token-id=${vaultRootToken}
        '';
        postStart = with infraLib; ''
          wait-for vault.localdomain:8200 -q
          # wait for complete unseal
          sleep 5
          export VAULT_ADDR=http://vault.localdomain:8200
          # set secrets kv store to v1
          vault secrets disable secret
          vault secrets enable -version=1 -path=secret kv
          ${createVaultPolicies (recursiveUpdate defaultVaultPolicies cfg.vaultPolicies)}
          ${writeVaultPaths "auth/token/roles/" (defaultVaultRoles // cfg.vaultRoles)}
          ${writeVaultPaths "" (defaultVaultData // cfg.vaultData)}
          # create token for vaulttmpfs
          vault token create -orphan -period=86400 -renewable=true -policy=applications_token_creator -metadata="applications=kubernetes-flexvolume-vault-plugin" -id ${vaultTmpfsToken}
        '';
      };

      systemd.services.consul = {
        wantedBy = [ "multi-user.target" ];
        after = [ "network-online.target" ];
        path = with pkgs; [ consul waitFor curl ];
        script = ''
          consul agent -dev -log-level info -client 169.254.1.11 -domain ${cfg.domain} -config-dir /etc/consul.d
        '';
        postStart = ''
          wait-for consul.localdomain:8500 -q
          ${concatStringsSep "\n"
            (mapAttrsToList (name: data:
              let dataFile = pkgs.writeText "${replaceStrings [ "/" ] [ "_" ] name}.json" (toJSON data);
              in "curl -X PUT -d @${dataFile} http://consul.localdomain:8500/v1/kv/${name}") cfg.consulData)}
        '';
      };

      environment.etc = {
        # We need consul.d to exist for consul to start
        "consul.d/dummy.json".text = "{ }";
      }
      # add externalService services in consul
      // (mapAttrs' (name: { address, port }:
        nameValuePair "consul.d/${name}.json" { text = toJSON { service = { inherit name port address; }; }; }
      ) cfg.externalServices);

      systemd.services.kube-bootstrap = {
        serviceConfig.Type = "oneshot";
        serviceConfig.RemainAfterExit = true;
        wantedBy = [ "kubernetes.target" ];
        after = [ "vault.service" "consul.service" "kube-apiserver.service" "kubelet-bootstrap.service" ];
        path = with pkgs; [ kubectl docker waitFor ];
        script = ''
          wait-for localhost:8080 -q -t 300
          # Even if the kube-apiserver is listening it might not
          # be bootstraped completely :/
          while ! kubectl api-resources | grep rolebinding
          do
            echo "Waiting on kube-apiserver to be provisioned..."
            sleep 1
          done
          # Give cluster-admin role to all accounts
          kubectl create clusterrolebinding permissive-binding \
            --clusterrole=cluster-admin \
            --user=admin \
            --user=kubelet \
            --group=system:serviceaccounts
          kubectl create -n kube-system secret generic calico-etcd-secrets \
            --from-file=etcd-ca=${certs.master}/ca.pem \
            --from-file=etcd-cert=${certs.master}/etcd.pem \
            --from-file=etcd-key=${certs.master}/etcd-key.pem
          kubectl apply -f /etc/kubernetes/infra/stage1
          # First wait on calico pods to be created
          while ! kubectl --namespace kube-system get pods | grep calico
          do
            echo "Waiting on calico pods..."
            sleep 1
          done
          nbPods=$(kubectl --namespace kube-system get pods | tail -n +2 | wc -l)
          while [ $(kubectl --namespace kube-system get pods | tail -n +2 | grep '1/1' | wc -l) -ne $nbPods ]
          do
            echo "Waiting on calico pods to be ready..."
            sleep 1
          done
          kubectl --namespace kube-system get pods
          kubectl apply -f /etc/kubernetes/infra/stage2
        '';
      };

    })

    # External services must be located on the master node for now.
    # Add a route on nodes to reach them
    (mkIf isNode {

      networking.interfaces.eth1.ipv4.routes = [
        { address = "169.254.1.0"; prefixLength = 24; via = cfg.masterIP; }
      ];

    })

    {

      networking = {
        firewall.enable = false;
        domain = cfg.domain;
        hosts = {
          "${cfg.masterIP}" = [ "api.${cfg.domain}" "etcd.${cfg.domain}" ];
          "169.254.1.10" = [ "resolver.localdomain" "resolver" ];
          "169.254.1.11" = [ "consul.localdomain" "consul" ];
          "169.254.1.13" = [ "vault.localdomain" "vault" ];
          "169.254.2.1" = [ "fluentd.localdomain" "fluentd" ];
        } // mapAttrs' (name: { address, ... }: nameValuePair address [ "${name}.localdomain" name ])
          cfg.externalServices;
        # fluentd is deployed on all nodes, add a local interface for it
        interfaces.lo.ipv4.addresses = [
          { address = "169.254.2.1"; prefixLength = 32; }
        ];
      };

      # dnsmasq will bind on all interfaces because it
      # sets 127.0.0.1 in resolv.conf
      services.dnsmasq = {
        enable = true;
        extraConfig = ''
          bind-interfaces
          no-negcache
          server=/localdomain/
          server=/node.${cfg.domain}/169.254.1.11#8600
          server=/query.${cfg.domain}/169.254.1.11#8600
          server=/service.${cfg.domain}/169.254.1.11#8600
        '';
      };

      services.kubernetes = {
        roles = cfg.roles;
        verbose = false;
        clusterCidr = null;
        caFile = "${certs.master}/ca.pem";
        etcd = {
          servers = ["https://etcd.${cfg.domain}:2379"];
          certFile = "${certs.worker}/etcd-client.pem";
          keyFile = "${certs.worker}/etcd-client-key.pem";
        };
        apiserver = {
          bindAddress = config.networking.primaryIPAddress;
          advertiseAddress = config.networking.primaryIPAddress;
          tlsCertFile = "${certs.master}/kube-apiserver.pem";
          tlsKeyFile = "${certs.master}/kube-apiserver-key.pem";
          kubeletClientCertFile = "${certs.master}/kubelet-client.pem";
          kubeletClientKeyFile = "${certs.master}/kubelet-client-key.pem";
          serviceAccountKeyFile = "${certs.master}/kube-service-accounts.pem";
          # enable PodPreset api
          runtimeConfig = "authentication.k8s.io/v1beta1=true,settings.k8s.io/v1alpha1=true";
          enableAdmissionPlugins = [ "NamespaceLifecycle" "LimitRanger" "ServiceAccount" "ResourceQuota" "DefaultStorageClass" "DefaultTolerationSeconds" "NodeRestriction" "PodPreset" ];
        };
        kubeconfig = {
          server = "https://api.${cfg.domain}";
        };
        kubelet = {
          tlsCertFile = "${certs.worker}/kubelet.pem";
          tlsKeyFile = "${certs.worker}/kubelet-key.pem";
          hostname = "${config.networking.hostName}.${cfg.domain}";
          kubeconfig = {
            certFile = "${certs.worker}/apiserver-client-kubelet-${config.networking.hostName}.pem";
            keyFile = "${certs.worker}/apiserver-client-kubelet-${config.networking.hostName}-key.pem";
          };
          networkPlugin = "cni";
          nodeIp = config.networking.primaryIPAddress;
          cni = {
            packages = with pkgs; [ cni_0_3_0 calicoCniPlugin ];
            config = [
              {
                name = "calico-k8s-network";
                cniVersion = "0.3.0";
                type = "calico";
                etcd_endpoints = "https://etcd.${cfg.domain}:2379";
                etcd_key_file = "${certs.master}/etcd-key.pem";
                etcd_cert_file = "${certs.master}/etcd.pem";
                etcd_ca_cert_file = "${certs.master}/ca.pem";
                log_level = "INFO";
                ipam = {
                  type = "calico-ipam";
                };
                policy = {
                  type = "k8s";
                };
                kubernetes = {
                  kubeconfig = pkgs.writeText "cni-kubeconfig" (builtins.toJSON {
                    apiVersion = "v1";
                    kind = "Config";
                    clusters = [{
                      name = "local";
                      cluster.certificate-authority = "${certs.master}/ca.pem";
                      cluster.server = "https://api.${cfg.domain}";
                    }];
                    users = [{
                      name = "kubelet";
                      user = {
                        client-certificate =
                          "${certs.worker}/apiserver-client-kubelet-${config.networking.hostName}.pem";
                        client-key =
                          "${certs.worker}/apiserver-client-kubelet-${config.networking.hostName}-key.pem";
                      };
                    }];
                    contexts = [{
                      context = { cluster = "local"; user = "kubelet"; };
                      current-context = "kubelet-context";
                    }];
                  });
                };
              }
              {
                name = "loopback";
                cniVersion = "0.3.0";
                type = "loopback";
              }
            ];
          };
          extraOpts = "--resolv-conf=/etc/kubernetes/kubelet/resolv.conf --volume-plugin-dir=/etc/kubernetes/volumeplugins";
        };
        controllerManager = {
          serviceAccountKeyFile = "${certs.master}/kube-service-accounts-key.pem";
          kubeconfig = {
            certFile = "${certs.master}/apiserver-client-kube-controller-manager.pem";
            keyFile = "${certs.master}/apiserver-client-kube-controller-manager-key.pem";
          };
        };
        scheduler = {
          kubeconfig = {
            certFile = "${certs.master}/apiserver-client-kube-scheduler.pem";
            keyFile = "${certs.master}/apiserver-client-kube-scheduler-key.pem";
          };
        };
        proxy = {
          enable = false;
          kubeconfig = {
            certFile = "${certs.worker}/apiserver-client-kube-proxy.pem";
            keyFile = "${certs.worker}/apiserver-client-kube-proxy-key.pem";
          };
        };
        addonManager.enable = false;
        addons.dns.enable = false;
      };

      virtualisation.dockerPreloader = {
        images = with pkgs.dockerImages; [
          kube2consulWorker
          pulled.calicoNodeImage
          calicoKubeControllers
          ] ++ cfg.seedDockerImages;
        qcowSize = 2000;
      };

      systemd.services.kubelet = {
        # For vaulttmpfs
        environment = {
          VAULTTMPFS_GENERATOR_TOKEN_PATH = pkgs.writeText "token" vaultTmpfsToken;
          VAULT_WRAP_TTL = "5m";
          VAULT_ADDR = "http://vault.localdomain:8200";
        };
      };

      systemd.services.fluentd = {
        wantedBy = [ "multi-user.target" ];
        after = [ "network.target" ];
        script = "${pkgs.fluentdCw}/bin/fluentd --no-supervisor -q -c ${infraConfig.fluentdConf}";
      };

      environment.etc = with infraConfig; {
        "docker/daemon.json".text = ''
          {
            "dns-opts": ["ndots:2"]
          }
        '';
        # provides our DNS to pods so that they can resolve
        # consul and local services.
        "kubernetes/kubelet/resolv.conf".text = ''
          search ${cfg.domain}
          nameserver 169.254.1.10
          options timeout:1
        '';
        # calico deployment
        "kubernetes/infra/stage1/resources.json".source = pkgs.lib.kubenix.buildResources {
          configuration = k8sStage1Resources;
          # don't include calico secrets resource since they are created manually in kube-bootstrap service
          resourceFilter = _: name: _: if name != "calico-etcd-secrets" then true else false;
        };
        # kube2consul deployment
        "kubernetes/infra/stage2/resources.json".source = pkgs.lib.buildK8SResources k8sStage2Resources;
        # vaulttmpfs plugin must be placed in a special directory tree so that the kubelet can
        # find it. This directory is passed to the kubelet with the --volume-plugin-dir flag.
        # FIXME: put the plugin in /usr/libexec/kubernetes/kubelet-plugins/volume/exec/
        "kubernetes/volumeplugins/cloudwatt~vaulttmpfs/vaulttmpfs".source =
          "${pkgs.vaulttmpfs}/bin/kubernetes-flexvolume-vault-plugin";
      };

      environment.systemPackages = with pkgs; [
        jq
        kubectl
        docker
        vault
        calicoctl
      ];

      environment.variables = {
        TERM = "xterm";
        # this is for using vault on the host
        VAULT_ADDR = "http://vault.localdomain:8200";
      };

    }

  ]);

}
